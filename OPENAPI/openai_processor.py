import os
import shutil
import tempfile
import glob
import subprocess
import json
import time
import logging
import uuid

# --- Configuration ---
WORKSPACE_NAME = "openai_api_workspace"
WORKSPACE_DIR = os.path.join(os.getcwd(), WORKSPACE_NAME)
# Define where shared/processed files from the *other* app might be, or use independent dirs
# Adjust these if your directory structure is different
SHARED_GRADIO_WORKSPACE_DIR = os.path.join(os.getcwd(), "api_workspace")
PROCESSED_PDF_DIR = os.path.join(SHARED_GRADIO_WORKSPACE_DIR, "processed_pdfs")
PROCESSED_JSONL_DIR = os.path.join(SHARED_GRADIO_WORKSPACE_DIR, "processed_jsonl")
PROCESSED_PREVIEW_DIR = os.path.join(SHARED_GRADIO_WORKSPACE_DIR, "html_previews")
# Dedicated directory for DOCX files generated by this specific API
OPENAI_DOCX_OUTPUT_DIR = os.path.join(WORKSPACE_DIR, "openai_docx_output")

def ensure_openai_dirs():
    """Ensures directories for the OpenAI compat app exist."""
    os.makedirs(WORKSPACE_DIR, exist_ok=True)
    os.makedirs(OPENAI_DOCX_OUTPUT_DIR, exist_ok=True)
    # Also ensure shared directories exist, as we might read/write to them
    os.makedirs(PROCESSED_PDF_DIR, exist_ok=True)
    os.makedirs(PROCESSED_JSONL_DIR, exist_ok=True)
    os.makedirs(PROCESSED_PREVIEW_DIR, exist_ok=True)

ensure_openai_dirs()

# --- Logging Setup ---
log_format = '%(asctime)s - %(name)s - %(levelname)s - [OpenAICompat] - %(message)s'
# Avoid duplicate handlers if basicConfig was called elsewhere (e.g., by the other app if run in same process)
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger(__name__)

# --- Conversion Script Helper (Adapted) ---
def run_conversion_script(script_name, input_path, output_dir):
    """Runs a conversion script, handling potential path issues."""
    # Try finding scripts relative to this file's location first
    script_dir_rel = os.path.join(os.path.dirname(__file__), "..", "scripts")
    # Then try finding scripts relative to CWD (if run from top level)
    script_dir_cwd = os.path.join("scripts")

    script_path_rel = os.path.join(script_dir_rel, script_name)
    script_path_cwd = os.path.join(script_dir_cwd, script_name)

    if os.path.exists(script_path_rel):
        script_path = script_path_rel
    elif os.path.exists(script_path_cwd):
        script_path = script_path_cwd
    else:
        raise FileNotFoundError(f"转换脚本未找到: 尝试了 {script_path_rel} 和 {script_path_cwd}")

    cmd = ["python", script_path, input_path, output_dir]
    logger.info(f"运行转换脚本: {' '.join(cmd)}")
    try:
        process = subprocess.run(
            cmd, capture_output=True, text=True, check=True, encoding='utf-8', timeout=120
        )
        logger.info(f"脚本 {script_name} STDOUT:\n{process.stdout}")
        if process.stderr:
             logger.warning(f"脚本 {script_name} STDERR:\n{process.stderr}")
        logger.info(f"脚本 {script_name} 执行成功。")
        # Return list of ALL files potentially created in the output dir
        return glob.glob(os.path.join(output_dir, "*"))
    except subprocess.CalledProcessError as e:
        logger.error(f"脚本 {script_name} 执行失败 (Code: {e.returncode}). STDOUT: {e.stdout} STDERR: {e.stderr}")
        raise RuntimeError(f"脚本 {script_name} 执行失败 (Code: {e.returncode}): {e.stderr.strip() if e.stderr else '(No stderr)'}") from e
    except subprocess.TimeoutExpired as e:
         logger.error(f"脚本 {script_name} 执行超时.")
         raise TimeoutError(f"脚本 {script_name} 执行超时.") from e
    except Exception as e:
        logger.exception(f"运行转换脚本时发生意外错误: {script_name}")
        raise e


# --- Core Processing Logic --- 
def process_pdf_openai_style(pdf_filepath):
    """
    Runs OLMOCR, yields a single <think> log block, then converts to DOCX,
    and finally yields the result dictionary.

    Yields:
        str: A single block of log messages wrapped in <think> tags.
        dict: The final result dictionary {status: ..., html_filename: ..., ...}.
    """
    run_dir = None
    persistent_pdf_path = None
    persistent_jsonl_path = None
    generated_docx_path = None
    generated_html_path = None
    current_file_name = os.path.basename(pdf_filepath)
    run_uuid = str(uuid.uuid4())
    task_id_for_logs = run_uuid[:8]
    log_accumulator = [] # <-- Accumulate logs here

    params = {
        'error_rate': 0.03,
        'max_retries': 5,
        'workers': 1
    }

    def log_and_accumulate(message):
        log_accumulator.append(message)
        # Log to file/console immediately
        logger.info(f"[OpenAI:{task_id_for_logs}] {message}")

    log_and_accumulate(f"任务 {task_id_for_logs}: 开始处理文件: {current_file_name} (使用预设参数)")

    try:
        # 1. Create temp dir
        run_dir = tempfile.mkdtemp(dir=WORKSPACE_DIR, prefix=f"openai_run_{run_uuid}_")
        log_and_accumulate(f"创建临时工作区: {run_dir}")

        # 2. Copy PDF
        base_name, _ = os.path.splitext(current_file_name)
        safe_base_name = f"openai_{run_uuid}_{''.join(c if c.isalnum() else '_' for c in base_name[:30])}"
        persistent_pdf_filename = safe_base_name + ".pdf"
        persistent_pdf_path = os.path.join(PROCESSED_PDF_DIR, persistent_pdf_filename)
        os.makedirs(PROCESSED_PDF_DIR, exist_ok=True)
        shutil.copy(pdf_filepath, persistent_pdf_path)
        log_and_accumulate(f"缓存 PDF 到: {persistent_pdf_path}")

        # 3. Run OLMOCR
        olmocr_results_dir = os.path.join(run_dir, "results")
        cmd = [
            "python", "-m", "olmocr.pipeline",
            run_dir, "--pdfs", persistent_pdf_path,
            "--max_page_error_rate", str(params['error_rate']),
            "--max_page_retries", str(params['max_retries']),
            "--workers", str(params.get('workers', 1))
        ]
        cmd_str = ' '.join(cmd)
        log_and_accumulate(f"执行 OLMOCR 命令: {cmd_str}")
        process = subprocess.run(cmd, capture_output=True, text=True, check=False, encoding='utf-8')
        log_and_accumulate(f"OLMOCR 进程完成 (返回码: {process.returncode})")
        if process.stdout: log_and_accumulate(f"OLMOCR STDOUT [截断]:\n{process.stdout.strip()[:500]}...")
        if process.stderr: log_and_accumulate(f"OLMOCR STDERR [截断]:\n{process.stderr.strip()[:500]}...")
        if process.returncode != 0:
            raise RuntimeError(f"OLMOCR failed (Code: {process.returncode}). Check logs for details.")

        # 4. Process JSONL
        jsonl_files_temp = glob.glob(os.path.join(olmocr_results_dir, "output_*.jsonl"))
        if not jsonl_files_temp:
            raise FileNotFoundError(f"OLMOCR output JSONL not found in {olmocr_results_dir}")
        temp_jsonl_path = jsonl_files_temp[0]
        if os.path.getsize(temp_jsonl_path) == 0:
             log_and_accumulate("警告 - OLMOCR 生成了空的 JSONL 文件。")
             raise RuntimeError("OLMOCR generated empty JSONL file.")
        os.makedirs(PROCESSED_JSONL_DIR, exist_ok=True)
        persistent_jsonl_filename = f"{safe_base_name}_output.jsonl"
        persistent_jsonl_path = os.path.join(PROCESSED_JSONL_DIR, persistent_jsonl_filename)
        shutil.copy(temp_jsonl_path, persistent_jsonl_path)
        log_and_accumulate(f"JSONL 文件已保存到: {persistent_jsonl_path}")

        # 5. Generate HTML Preview
        os.makedirs(PROCESSED_PREVIEW_DIR, exist_ok=True)
        viewer_cmd = [
            "python", "-m", "olmocr.viewer.dolmaviewer",
            persistent_jsonl_path, "--output_dir", PROCESSED_PREVIEW_DIR
        ]
        log_and_accumulate("执行 HTML 预览生成...")
        viewer_process = subprocess.run(viewer_cmd, capture_output=True, text=True, check=False, encoding='utf-8')
        log_and_accumulate(f"HTML 预览生成完成 (Code: {viewer_process.returncode})")
        if viewer_process.stdout: log_and_accumulate(f"Viewer STDOUT [截断]:\n{viewer_process.stdout.strip()[:500]}...")
        if viewer_process.stderr: log_and_accumulate(f"Viewer STDERR [截断]:\n{viewer_process.stderr.strip()[:500]}...")
        generated_html_path = None
        if viewer_process.returncode == 0:
            path_for_html_name = persistent_pdf_path.replace(os.sep, '_').replace('.', '_')
            expected_html_filename = path_for_html_name + ".html"
            expected_html_path = os.path.join(PROCESSED_PREVIEW_DIR, expected_html_filename)
            if os.path.exists(expected_html_path):
                generated_html_path = expected_html_path
                log_and_accumulate(f"HTML 预览文件找到: {os.path.basename(generated_html_path)}")
            else:
                log_and_accumulate(f"警告 - 未找到预期的 HTML 文件 {expected_html_filename}")
                logger.warning(f"[OpenAI:{task_id_for_logs}] Expected HTML file not found: {expected_html_path}")
        else:
            log_and_accumulate("警告 - 生成 HTML 预览失败。")
            logger.warning(f"[OpenAI:{task_id_for_logs}] HTML preview generation failed. Code: {viewer_process.returncode}")

        # 6. Convert to DOCX
        log_and_accumulate("开始转换为 DOCX...")
        script_name = "jsonl_to_docx.py"
        os.makedirs(OPENAI_DOCX_OUTPUT_DIR, exist_ok=True)
        try:
            generated_files = run_conversion_script(script_name, persistent_jsonl_path, OPENAI_DOCX_OUTPUT_DIR)
            log_and_accumulate("DOCX 转换脚本执行完成。")
            docx_filename = f"{os.path.splitext(os.path.basename(persistent_jsonl_path))[0]}.docx"
            possible_docx_path = os.path.join(OPENAI_DOCX_OUTPUT_DIR, docx_filename)
            if os.path.exists(possible_docx_path):
                generated_docx_path = possible_docx_path
                log_and_accumulate(f"DOCX 文件找到: {os.path.basename(generated_docx_path)}")
            else:
                 docx_files_in_list = [f for f in generated_files if f.endswith('.docx')]
                 if docx_files_in_list:
                      generated_docx_path = docx_files_in_list[0]
                      log_and_accumulate(f"DOCX 文件找到 (来自脚本): {os.path.basename(generated_docx_path)}")
                 else:
                    log_and_accumulate(f"警告 - 未找到生成的 DOCX 文件 ({docx_filename})。")
                    logger.warning(f"[OpenAI:{task_id_for_logs}] Could not find generated DOCX file: {docx_filename} or in {generated_files}")
        except FileNotFoundError as script_e:
             log_and_accumulate(f"错误 - DOCX 转换脚本 ({script_name}) 未找到: {script_e}")
        except Exception as docx_e:
            log_and_accumulate(f"错误 - DOCX 转换失败: {docx_e}")
            logger.error(f"[OpenAI:{task_id_for_logs}] DOCX conversion failed: {docx_e}", exc_info=True)

        log_and_accumulate("处理完成。")

        # --- Yield accumulated thoughts --- 
        yield f"<think>\n{"".join([f'{log}\n' for log in log_accumulator])}</think>"

        # --- Yield final result --- 
        yield {
            "status": "success",
            "html_filename": os.path.basename(generated_html_path) if generated_html_path else None,
            "docx_filename": os.path.basename(generated_docx_path) if generated_docx_path else None,
            "html_dir": PROCESSED_PREVIEW_DIR,
            "docx_dir": OPENAI_DOCX_OUTPUT_DIR
        }

    except Exception as e:
        error_message = f"处理文件 {current_file_name} 时发生严重错误: {e}"
        log_and_accumulate(f"严重错误 - {error_message}")
        logger.exception(f"[OpenAI:{task_id_for_logs}] Critical error processing file {current_file_name}")

        # --- Yield accumulated thoughts before error (if any) --- 
        # Ensure logs are yielded even on critical failure
        yield f"<think>\n{"".join([f'{log}\n' for log in log_accumulator])}</think>"

        # --- Yield final error result --- 
        yield {"status": "error", "message": error_message}

    finally:
        if run_dir and os.path.exists(run_dir):
            try:
                shutil.rmtree(run_dir)
                logger.info(f"[OpenAI:{task_id_for_logs}] Cleaned up temp dir: {run_dir}")
            except OSError as e:
                logger.error(f"[OpenAI:{task_id_for_logs}] Failed to remove temp dir {run_dir}: {e}") 